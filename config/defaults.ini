[output]

#Output CSVs cluster_synthesis and cluster_detail
do_output_main_analysis=yes

# Output integrated knowledge graph predicate provenances. NO, ORIGINAL (show original atts), TAGGED (show tagged extractions)
ikgpp_mode=ORIGINAL

# If do_print_cluster_analysis=yes, outputs a more detailed analysis on cluster ids specified on clusters_to_compare
# Eg: clusters_to_compare=1,2-3 --> outputs a file for analysis on cluster 1, and 1 file for cluster 2 and 3 AND
# inter-cluster edges between 2 and 3

# Note that IDs can change from 1 launch to another of algorithm, so you have to:
# - launch step 1
# read on output the IDs of interesting clusters, modify this parameter and then launch step 2
# (it does not launch algorithm again, but just the analysis)
clusters_to_compare=

# Make an evaluation according of results on synthetic input
# NOTE: evaluation on real input is not available currently
do_synthetic_evaluation=no

# Build a separated file for isolated attributes
do_separate_isolated=no

# Output some detailed information about specific NODES.Analogue to specific cluster analysis
# Format: site_domain/AttributeName1,site_domain2/AttributeName2,...
specific_nodes_camera=

#If debug mode is activated, some additional data are put in output
debug_mode=no

# Additional details for tagging
debug_tags=no

[algorithm]

# If attributes match for at least one value with frequency < this (ratio of source attributes in which it is
# present), then
max_frequency_single_value=0.1

# Minimum number of common values for specifications in linkage between 2 attributes to select them as candidates
min_common_values_attributes=2

# Generated attributes can be put in a cluster even if they appear in same page with another cluster attribute
exclude_same_page_for_generated=no

# Clustering algorithm:
# - LOUVAIN: https://en.wikipedia.org/wiki/Louvain_Modularity
# - CC: just uses connected components
# - AGGLO: like CC, but avoids 2 nodes of same source to be in same cluster, removing the lowest possible edge
#   - EG: s1/A --0.5-- s2/B --0.2-- s3/C --0.9-- s1/X <==> remove s2/b -- s3/c edge <==> C1 = {s1/A: s2/B}; C2 = {s3/C; S1/X}
clustering_algorithm=LOUVAIN

# If YES, uses as prior a value vector similarity between 2 atts domain, taking into account the TF-IDF
# TODO: better define this measure
use_idf=yes

# See RecordLinkageMethod on bdsa_config
record_linkage_method=PROB
record_linkage_behavior=BOTH
# Reinitialize record linkage at the beginning of each schema alignment-record linkage phase
restart_linkage=no

# If yes, previous clusters are kept in iterations, only for original attributes
keep_previous_original_clusters=yes

# If true, family linkage can link pages from same source
allow_linkage_same_source=no
default_linkage_weight=0.9
number_of_iterations=2

# Stop linkage-alignment iteration when
# - minimum elements in identical clusters from old to current are bigger than a threshold (sort of blocking for stop condition)
# - linkage clustering similarity to previous one is at least this number
min_clustering_similarity_to_stop_iteration=0.98
min_elements_in_identical_clusters_to_stop_iteration=0.92

# Same as before but for alignment - tagging steps.
# Stops when dictionary extracted for tagging are too similar
min_dict_similarity_to_stop_iteration=0.95
min_terms_in_identical_entries_to_stop_iteration=0.9

# If a classifier is used to compute record linkage, specify here how many negative samples should be provided for each positive
neg_sample_linkage=10

# Minimum blocking score to select pair of attributes for linkage.
# Currently corresponds to number of matching values for aligned (significant) attributes
min_blocking_score_linkage=2

# the more values are present in a sa the more probable there will be an error.
# here we must set maximum error rate when nb of values tends to infinity.
error_rate_per_value= 0.05

# Default a-prior for attribute match probability
default_apriori_equivalence_ratio=0.01
# Same for linkage
default_apriori_linkage_ratio=0.01

#Prior = name_similarity * similarity_names_weight + domain_similarity * (1 - similarity_names_weight)
similarity_names_weight=0.1

# Edges with a weight below this thresholds will be deleted before clustering
# (NOT during iterations of schema alignment - linkage accuracy)
min_edge_weight = 0.2

# While comparing attributes, if a product ID is present in more specifications of a source,  then use the most frequent value.
# If NO, then compute cartesian product of values in specifications in source A and B, and weight each as min(a,b)/(a*b)
# (where a*b is the size of cartesian product)
# no_minweight: weight them as 1/(a*b)
one_value_per_product=no_minweight

# Same for linkage
min_edge_weight_linkage = 0.2

# IF yes, use the original linkage even in further iterations, otherwise use the last created linkage
use_original_linkage_for_training_in_iteration=yes

# If a token is present in at least this ratio of occurrences for an attribute, then it is removed for matching.
# 0 = exclude rule
common_token_ratio=0

# Exclude generated attributes when analyzing pair of specifications in family linkage.
# Better convergence of attribute but worse precision
exclude_generated_from_family_linkage=no

# Delete clusters containing only virtual attributes, whose original attributes are already in the same clusters
delete_subclusters_only_virtuals=no

# If no, the matching score between two attributes will just consider prior probability (i.e. similarity of general
# domain and domain in linkage) and ignore posterior (equivalence of values for products in linkage)
compute_posterior_probability=yes

[tagging]
extract_dict_values_from=NON_ISOLATED
attributes_to_tag=TAIL_CLUSTERS

# Min and max length of values used as potential tags
max_len_atomic_values = 25
min_len_atomic_values = 2
# Max frequency of a provided value to be considered as potential tag
max_ratio_occs = 0.02
# Range of ngrams that will be looked for in complex attributes
min_ngram=1
max_ngram=10
# If yes, each extracted snippet is possibly assigned to only 1 cluster (the one with less extracted snippet in value),
# provided the other clustershave at least 1 other snippet left
assign_snippets_one_cluster=yes

# If yes, every n-gram detected in a value is tagged. If false, once an n-gram is associated to one or more clusters, it is then deleted
# E.g with value="A B C D" and dictionary 1-AB, 2-BC, 3-B, 4-D. If YES, all 4 dictionary terms are used for tagging.
# If FALSE, the only tags would be A B and D.
tag_every_combination=no

# We keep only values present in at least MIN_ATT_IN_CLUSTER attributes of a cluster
min_att_in_cluster=1

# Threshold for generated attributes
min_value_ratio_on_original_attribute=0.1
min_values=3
# Score is sum of, for each distinct value, 1/[nb_clusters_the_value_occurs]
# E.g. attributes should have 1 distinct value present in only 1 cluster, or 2 present each in 2 clusters ecc.
min_distinct_values_score=1

# If yes, the atomic values used for tagging are extracted only from HEAD clusters. Otherwise, they are extracted from all clusters.
extract_atomic_values_only_from_head_clusters=no

#  If true, highlights the substrings tagged on the example values of attributes in output.
add_extraction_data_in_output=no

# Token to be kept should have at least [min_mi] MI, and should co-occur in at least [min_sa_pair] sa pairs each for at least
# [min_page_pair]
min_mi=1
min_sa_pair=3
min_page_pair=3

# Minimum number of significant tokens in a cluster to keep that cluster
min_significant_tokens=3

# Minimum number of distinct values in generated attributes m.i.
min_distinct_values_generated_mi=3

[master_source]
source_name=
source_attributes=
excluded_source_attributes=
min_size_external_clusters=10

[inputs]

specifications_source=FILE

specifications_full_path=${local:input_path}/dataset/${specifications}
specifications=specifications_prod

# A dataset can have multiple linkages available, so at a particular launch the linkage to use must be specified.
# In order to distinguish between different linkages, a suffix may be used in linkage JSON files.
# In particular, default JSON linkage file can be found in path [source]/[cat]_linkage.json, while variations in
# [source]/[cat]_linkage_[suffix].json . Here the suffix to use should be provided, or left empty to use default linkage
linkage_suffix=

# Linkage file in original Dexter format
linkage_dexter_full_path=${local:input_path}/linkage/${linkage_dexter}
linkage_dexter=id2category2urls.json
community_linkage_dexter_full_path=${local:input_path}/linkage/${community_linkage_dexter}
community_linkage_dexter=id2comm2urls.json
linkage_dexter_combined_clean_full_path=${local:input_path}/linkage/${linkage_dexter_combined_clean}
linkage_dexter_combined_clean=cat2id2comm2urls_clean.json
linkage_dexter_combined_full_path=${local:input_path}/linkage/${linkage_dexter_combined}
linkage_dexter_combined=cat2id2comm2urls_full.json

# Exclude a list of attribute names from importing
excluded_attribute_names=
category=

# Location of ground truth
ground_truth_file=${specifications}_gt.csv
ground_truth_path=${local:input_path}/ground_truth/${ground_truth_file}

# Location of ground truth
ground_truth_instance_level_file=${specifications}_gt_instance.csv
ground_truth_instance_level_path=${local:input_path}/ground_truth/${ground_truth_instance_level_file}

[simulation]
### ALTER INPUT DATA TO SIMULATE PARTICULAR SITUATIONS FOR EXPERIMENTS ###

# How many page pairs in linkage to add as a % to existing linkage pairs.
# For instance, is there are 10 pairs and this is 1.5, we will add 15 pairs, thus making 25 pairs as a total
# Notice that there might be more than that, because of transitive closures
ratio_linkage_added=0

# If yes, pages without initial linkage are eliminated from input
delete_pages_without_linkage=no

# Rule for deleting sources: RANDOM, KEEP_MORE_LINKAGE, KEEP_LESS_LINKAGE, NONE
source_removal_rule=NONE
#Only this number of sources from input will be kept, the other will be deleted.
number_sources_keep=0
#List of sources that will be kept anyhow (tipically because they are needed for evaluation
sources_mandatory=

#Only this ratio of linkages from input will be kept, the other will be deleted.
percentage_linkage_kept=1
#Rule for deleting linkage: RANDOM, KEEP_BIG_CLUSTERS, KEEP_SMALL_CLUSTERS, NONE
linkage_removal_rule=NONE

[experiments]
attribute_cardinality=no
attribute_size=no
cluster_distinct_sources=no
attribute_linkage=no
source_size=no
source_linkage=no

[local]
### INTERN CONFIGS ###
output_path=/home/federico/BDSA/data/analysis_output

# Algorithm experiment results will be put here (ie output with precision-recall and other data). See main_evaluations
experiments_output_path=/home/federico/BDSA/experiments
input_path=/home/federico/BDSA/data
cache_path=/home/federico/BDSA/cache

# Location of repository containing algorithm paramters, used by automatic_evalution_launcher (cf file for details)
algo_parameters_repository_path=/home/federico/BDSA/dexter_analysis_config

mongo_host=localhost:27017
mongo_db=SyntheticDataset
connection_string=${connection_string_local}
;connection_string=${connection_string_tunnel}

### INTERN CONFIGS ###
connection_string_local= dbname='${db_name}' user='${user}' host='${remote_host}' password='${password}'
connection_string_tunnel= dbname='${db_name}' user='${user}' host='${local_host}' password='${password}'
db_name=bdsa
user=postgres
remote_host=sinai.inf.uniroma3.it
local_host=127.0.0.1
;Password is not stored here, use algo_parameters.ini configuration file
;password=XXX